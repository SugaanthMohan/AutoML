[EMAIL_NOTIFIER]
FROM=AutoMLNotifier@gmail.com
TO=sugaanth.mohan@gmail.com
CC=sugaanth.mohan@gmail.com
SUBJECT_PREFIX=AutoML - (?Status) | (?Subject)
HOST=localhost

[LOGGER]
# TYPE : OPTION = (TEMP, FILESYSTEM)
TYPE=TEMP
DIR=AutoML
PREFIX=AutoML_(?Timestamp).log

[AUTOML]
CONFIGURATION=AUTOML_CONFIGURATIONS
MODELS=sklearn.linear_model.ElasticNetCV
       sklearn.ensemble.ExtraTreesRegressor
       sklearn.ensemble.GradientBoostingRegressor
       sklearn.ensemble.AdaBoostRegressor
       sklearn.tree.DecisionTreeRegressor
       sklearn.neighbors.KNeighborsRegressor
       sklearn.linear_model.LassoLarsCV
       sklearn.svm.LinearSVR
       sklearn.ensemble.RandomForestRegressor
       sklearn.linear_model.RidgeCV
       xgboost.XGBRegressor
       sklearn.linear_model.SGDRegressor

[AUTOML_CONFIGURATIONS]
generations=3
population_size=20
verbosity=2
use_dask=1
warm_start=0
scoring=r2
n_jobs=-1
config_dict=

[sklearn.linear_model.ElasticNetCV]
l1_ratio=[0.0, 0.05, 0.1, 0.15000000000000002, 0.2, 0.25, 0.30000000000000004, 0.35000000000000003, 0.4, 0.45, 0.5, 0.55, 0.6000000000000001, 0.65, 0.7000000000000001, 0.75, 0.8, 0.8500000000000001, 0.9, 0.9500000000000001, 1.0]
tol=[1e-5, 1e-4, 1e-3, 1e-2, 1e-1]

[sklearn.ensemble.ExtraTreesRegressor]
n_estimators=[100]
max_features=[0.05, 0.1, 0.15000000000000002, 0.2, 0.25, 0.3, 0.35000000000000003, 0.4, 0.45, 0.5, 0.55, 0.6000000000000001, 0.6500000000000001, 0.7000000000000001, 0.7500000000000001, 0.8, 0.8500000000000001, 0.9000000000000001, 0.9500000000000001, 1.0]
min_samples_split=[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
min_samples_leaf=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
bootstrap=[True,False]

[sklearn.ensemble.GradientBoostingRegressor]
n_estimators=[100],
loss=["ls", "lad", "huber", "quantile"]
learning_rate=[1e-3, 1e-2, 1e-1, 0.5, 1.]
max_depth=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
min_samples_split=[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
min_samples_leaf=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
subsample=[0.05, 0.1, 0.15000000000000002, 0.2, 0.25, 0.3, 0.35000000000000003, 0.4, 0.45, 0.5, 0.55, 0.6000000000000001, 0.6500000000000001, 0.7000000000000001, 0.7500000000000001, 0.8, 0.8500000000000001, 0.9000000000000001, 0.9500000000000001, 1.0]
max_features=[0.05, 0.1, 0.15000000000000002, 0.2, 0.25, 0.3, 0.35000000000000003, 0.4, 0.45, 0.5, 0.55, 0.6000000000000001, 0.6500000000000001, 0.7000000000000001, 0.7500000000000001, 0.8, 0.8500000000000001, 0.9000000000000001, 0.9500000000000001, 1.0]
alpha=[0.75, 0.8, 0.85, 0.9, 0.95, 0.99]

[sklearn.ensemble.AdaBoostRegressor]
n_estimators=[100],
learning_rate=[1e-3, 1e-2, 1e-1, 0.5, 1.]
loss=["linear", "square", "exponential"]

[sklearn.tree.DecisionTreeRegressor]
max_depth=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
min_samples_split=[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
min_samples_leaf=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]

[sklearn.neighbors.KNeighborsRegressor]
n_neighbors=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]
weights=["uniform", "distance"]
p=[1, 2]

[sklearn.linear_model.LassoLarsCV]
normalize=[True, False]

[sklearn.svm.LinearSVR]
loss=["epsilon_insensitive", "squared_epsilon_insensitive"]
dual=[True, False]
tol=[1e-5, 1e-4, 1e-3, 1e-2, 1e-1]
C=[1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1., 5., 10., 15., 20., 25.]
epsilon=[1e-4, 1e-3, 1e-2, 1e-1, 1.]

[sklearn.ensemble.RandomForestRegressor]
n_estimators=[100]
max_features=[0.05, 0.1, 0.15000000000000002, 0.2, 0.25, 0.3, 0.35000000000000003, 0.4, 0.45, 0.5, 0.55, 0.6000000000000001, 0.6500000000000001, 0.7000000000000001, 0.7500000000000001, 0.8, 0.8500000000000001, 0.9000000000000001, 0.9500000000000001, 1.0]
min_samples_split=[2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
min_samples_leaf=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
bootstrap=[True, False]

[sklearn.linear_model.RidgeCV]

[xgboost.XGBRegressor]
n_estimators=[100]
max_depth=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]
learning_rate=[1e-3, 1e-2, 1e-1, 0.5, 1.]
subsample=[0.05, 0.1, 0.15000000000000002, 0.2, 0.25, 0.3, 0.35000000000000003, 0.4, 0.45, 0.5, 0.55, 0.6000000000000001, 0.6500000000000001, 0.7000000000000001, 0.7500000000000001, 0.8, 0.8500000000000001, 0.9000000000000001, 0.9500000000000001, 1.0]
min_child_weight=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]
n_jobs=[1]
verbosity=[0]
objective=['reg:squarederror']

[sklearn.linear_model.SGDRegressor]
loss=['squared_loss', 'huber', 'epsilon_insensitive']
penalty=['elasticnet']
alpha=[0.0, 0.01, 0.001]
learning_rate=['invscaling', 'constant']
fit_intercept=[True, False]
l1_ratio=[0.25, 0.0, 1.0, 0.75, 0.5]
eta0=[0.1, 1.0, 0.01]
power_t=[0.5, 0.0, 1.0, 0.1, 100.0, 10.0, 50.0]
